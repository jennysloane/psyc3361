# Study 1 (aka Leverhulme Study 8) 


#The goal of checkpoint is to solve the problem of package reproducibility in R. Specifically, checkpoint solve the problems that occur when you donâ€™t have the correct versions of R packages. Since packages get updated on CRAN, it can be difficult to recreate an environment where all your packages are consistent with some earlier state.
#To solve this, checkpoint allows you to install packages from a specific snapshot date. 
install.packages("checkpoint") #install
library(checkpoint) #load
checkpoint("2020-11-20") #set date


#INSTALL PACKAGES  

install.packages("httr")        # 'httr' is used for downloading data from OSF
install.packages("qualtRics")    #qualtRics is designed to read .csv files generated by Qualtrics  - I used this package because it overcomes one of the big issues with opening Qualtrics .csv files in R (i.e., that rows 2 and 3 in Qualtrics files are text, therefore the usual 'read.csv' command reads the colums as 'characters' or 'string' rather than 'numeric' http://adrianbruegger.com/import-qualtrics-csv-files/)    Full details https://cran.csiro.au/web/packages/qualtRics/qualtRics.pdf 
install.packages("tidyr")    #tidyr for splitting column using delimiter 
install.packages("reshape2")   #reshape2 used for melting data to long Format 
install.packages("psych")    #for summary statistics 
install.packages("yarrr")    #yarrr for creating pirate plot 
install.packages("car")     #car for Anova (with type 3 SS)
install.packages("lsr")     #lsr for Cohens D
install.packages("devtools")
install.packages("ggpubr")
install.packages("effectsize")
install.packages("BayesFactor") #for Bayesian Anova and t-tests
install.packages("foreign") #for ordinal regression
install.packages("ggplot2")
install.packages("MASS")
install.packages("Hmisc")
install.packages("reshape2")

#LOAD PACKAGES 

library("qualtRics")
library("httr")
library("tidyr")
library("reshape2")
library("psych")
library("yarrr")
library("car")
library("lsr")
library("devtools")
library("ggpubr")
library("effectsize")
library("BayesFactor")

####FETCH THE DATA FROM OSF#### 

#The line below downloads the data csv file from oSF to your working directory, saved as "study1.csv"  credit: https://twitter.com/lakens/status/839469115253325824   
data<-GET('https://osf.io/tna7m/?action=download', write_disk('study1.csv', overwrite = TRUE))


#The line below reads the data from your working directory using the qualtRics 'readSurvey' function. This function strips out the text in rows 2 and 3 (among other useful things)
mydata=readSurvey("study1.csv")



# CONVERT mydata to a data.frame 
#'mydata' is a tibble rather than a data.frame (this is due to the way that the qualtRics package Formats the data)
#The code below converts mydata into a data.frame (**doing this this avoids problems further down the line)
mydata<- as.data.frame(mydata)
mydata

#count how many times the survey was opened (i.e., anyone with progress >= 1%)   [371] 
length(which(mydata$Progress>=1))


#Count how many particpants CONSENTED to take part (this is BEFORE any exclusions for non-completion / non-serious responding) [363]
#371 rows in data frame, but only 363 clicked the consent button 
length(which(mydata$Consent==1))

#OPTIONAL: the line below identifies the types of variable we have (e.g., numeric, string, character etc.) - Just to check that qualtRics readSurvey has worked properly (i.e. to check whether it has set all relevant columns to 'numeric') 
sapply(mydata, class)



#rename variables that have odd names 
colnames(mydata)[colnames(mydata)=="SC0"] <- "recall_score"
colnames(mydata)[colnames(mydata)=="FL_10_DO"] <- "condition"





####UNPLANNED EXCLUSIONS (NOT PRE-REGISTERED)####
#SOME PARTICPANTS COMPLETED THE STUDY TWICE SO WE cHOSE TO EXCLUDE THEIR SECOND ATTEMPT BASED ON THEIR Prolific_PID
#This is because a number of participants recieved an error message on study completion - some of them decided to complete the study a second time. 
#The folowing procedure is taken from https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/#:~:text=Remove+duplicate+rows+in+a+data+frame,R+base+function+unique()+ 

#Identify whether any duplicates Prolific_PIDs exist  
duplicated(mydata$Prolific_PID)

#list the duplicate IDs   [59 found]
mydata$Prolific_PID[duplicated(mydata$Prolific_PID)]

#remove duplicate rows from mydata. This always removes the second (duplicate) response   
mydata<- mydata[!duplicated(mydata$Prolific_PID), ]

#we now have 312 rows (59 were removed)  
length(which(mydata$Progress>=1))



####PLANNED EXCLUSIONS####
#EXCLUDE PARTICIPANTS IF THEY DON'T MEET PREREGISTERED INCLUSION CRITERIA 
#create a new data.frame that is a SUBSET of the original. This data.frame...
#1) only includes SUBSET of participants who finished the study (Finished==1) AND declared that they answered seriously (seriousness_check==1) AND scored 4 or above on recall - i.e., excluding non-completers, non-serious responses and those who did not recall 4 or more items  
#2) only includes SUBSET of relevant columns required for analysis

mydata <- subset(mydata, Finished==1 & Serious_check==1 & recall_score>=4, select=c(Finished, `Duration (in seconds)`, Gender, Age, Serious_check, recall_score, condition, contradiction_1:advancement))

#Count how many particpants remain after exclusions (final sample size = 294)
length(which(mydata$Serious_check==1))

#Calculate mean completion time in seconds 
mean(mydata$`Duration (in seconds)`) #508.8197
#Convert to minutes 
508.8197/60 #8.480328

#OPTIONAL: export the data to a .csv to check it all looks in order (i.e. correct rows and columns retained? / only those who completed seriously retained?)
write.csv(mydata, file = "MyDataTidied.csv")



####DEMOGRAPHICS####

#count up how many pps assigned to each of the four conditions 
length(which(mydata$condition=="Block_1_Generic_Conflict"))   
length(which(mydata$condition=="Block_2_Generic_Consistent"))   
length(which(mydata$condition=="Block_3_Qualified_Conflict"))    
length(which(mydata$condition=="Block_4_Qualified_Consistent"))   


#total n (after exclusions)
nrow(mydata)
Total_n <- nrow(mydata)

#Age
mean(mydata$Age,na.rm=TRUE)
sd(mydata$Age,na.rm=TRUE)
min(mydata$Age,na.rm=TRUE)
max(mydata$Age,na.rm=TRUE)

#Gender 
#count of Males  
length(which(mydata$Gender==1))
Males<- length(which(mydata$Gender==1))
PercentMale <- (Males/Total_n)*100
#count of Females  
length(which(mydata$Gender==2))
Females<- length(which(mydata$Gender==2))
PercentFemale <- (Females/Total_n)*100
#count of 'Other'
length(which(mydata$Gender==3))
Other<- length(which(mydata$Gender==3))
PercentOther <- (Other/Total_n)*100
#count of 'Prefer not to say'
length(which(mydata$Gender==4))
Prefer_not_say <-length(which(mydata$Gender==4))
PercentPreferNot <- (Prefer_not_say/Total_n)*100
#print counts
Total_n
Males
Females
Other
Prefer_not_say
#print percentages
PercentMale
PercentFemale
PercentOther
PercentPreferNot


#create sepearte columns to identify levels of each IV  (original 'condition' Format is "Block_1_Generic_Conflict" - this just splits into four speerate columns using underscore as delimiter  - 'block' and 'number' are meaningless - Format and Conflict are the IVs)

mydata<- separate(data = mydata, col = condition, into = c("block", "number", "Format", "Conflict"))


#look at the results of this command 
write.csv(mydata, file = "MyDataTidied.csv")




#rename  (this is for the benefit of the pirate plot below to avoid overwriting )
mydata$Conflict <- gsub("Conflict", "Conf.", mydata$Conflict)
mydata$Conflict <- gsub("Consistent", "Non-Conf.", mydata$Conflict) 






#set these new IV columns as factors 
mydata$Format <- as.factor(mydata$Format)
levels(mydata$Format)

mydata$Conflict <- as.factor(mydata$Conflict)
levels(mydata$Conflict)






#sum the contradiction scores  

#create a new variable called 'contradiction' that is the sum of the six contradition ratings 
mydata$contradiction <- mydata$contradiction_1 + mydata$contradiction_2 + mydata$contradiction_3 + mydata$contradiction_4 + mydata$contradiction_5 + mydata$contradiction_6 

#look at the results of these command (will be a  new column called contradiction )
write.csv(mydata, file = "MyDataTidied.csv")




####pirate plots  / descriptives ####


par(mfrow = c(2, 2)) #will pace plots in a grid with 2 rows and 2 columns 


pirateplot(formula = contradiction ~ Conflict*Format, data = mydata,inf.method = 'ci', yaxt = "n", ylim =c(0,30), theme=1,main = "Contradiction", ylab = "Percieved Contradiction",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 0, to = 30, by = 5)) 


pirateplot(formula = advancement ~ Conflict*Format, data = mydata,inf.method = 'ci', yaxt = "n", theme=1,main = "Advancement", ylab = "Percieved Scientific Advancement",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = -1, to = 1, by = 1))


pirateplot(formula = confusion ~ Conflict*Format, data = mydata,inf.method = 'ci', yaxt = "n", theme=1,main = "Confusion", ylab = "Percieved Confusion",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 1, to = 5, by = 1)) 


#to set the grid back to default (just one plot) use the command below 
par(mfrow = c(1, 1))






#print descriptives that pirate plots are based on
contradiction.pp <- pirateplot(formula = contradiction ~ Conflict*Format,data = mydata,inf.method = 'ci',plot = FALSE)
contradiction.pp

confusion.pp <- pirateplot(formula = confusion ~ Conflict*Format,data = mydata,inf.method = 'ci', plot = FALSE)
confusion.pp

advancement.pp <- pirateplot(formula = advancement ~ Conflict*Format,data = mydata,inf.method = 'ci',plot = FALSE)
advancement.pp









####Inferential stats####  
#2x2 ANOVA  (using car package: car  uses Type 3 SS to be consistent with SPSS)   https://www.rdocumentation.org/packages/car/versions/1.0-7/topics/Anova  

#Conflict and Format were set as.factors earlier 

#this line turns off scientific notation for p values 
options(scipen=999) 



#This guide tells us how to get the same output values as SPSS (Type III SS). It involves changing the default contrast settings and using type III sum of squares http://www.statscanbefun.com/rblog/2015/8/27/ensuring-r-generates-the-same-anova-f-values-as-spss 
#It first tells us change default contrasts using the command below   
options(contrasts = c("contr.helmert", "contr.poly"))


#contradiction ANOVA
contradictionlm <- lm(contradiction ~ Conflict*Format, data = mydata)   #we first create a linear model 
Anova(contradictionlm,type=3)    #we then use the car package calculates Anova (with type 3 SS) on the linear model we have just created  
cohens_f(contradictionlm, partial = TRUE, ci = 0.95, squared = FALSE, model2 = NULL)

#Bayesian ANOVA 
contradiction_bayes <- anovaBF(contradiction ~ Conflict*Format, data = mydata)
contradiction_bayes # lists the Bayes factor for each model against the null model

#whichModels = "all" computes Bayes factors for all models
contradiction_bayes_all <- anovaBF(contradiction ~ Conflict*Format, whichModels = "all", data = mydata)
contradiction_bayes_all 


#exploritory simple main effects 
t.test(contradiction ~ as.factor(Conflict), data=mydata[which(mydata$Format=='Generic'),])
cohensD(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])  #d=3.66

t.test(contradiction ~ as.factor(Conflict), data=mydata[which(mydata$Format=='Qualified'),])
cohensD(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])    #d=2.74

#Bayesian t-tests for simple main effects
ttestBF(formula = contradiction ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])

ttestBF(formula = contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])




#confusion ANOVA
confusionlm <- lm(confusion ~ Conflict*Format, data = mydata)   #we first create a linear model 
Anova(confusionlm,type=3)    #we then use the car package calculates Anova (with type 3 SS) on the linear model we have just created  
cohens_f(confusionlm, partial = TRUE, ci = 0.95, squared = FALSE, model2 = NULL)

#Bayesian ANOVA 
confusion_bayes <- anovaBF(confusion ~ Conflict*Format, data = mydata)
confusion_bayes # lists the Bayes factor for each model against the null model

#whichModels = "all" computes Bayes factors for all models
confusion_bayes_all <- anovaBF(confusion ~ Conflict*Format, whichModels = "all", data = mydata)
confusion_bayes_all 


#exploritory simple main effects 
t.test(confusion ~ as.factor(Conflict), data=mydata[which(mydata$Format=='Generic'),])
cohensD(confusion ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])   #d=1.24

t.test(confusion ~ as.factor(Conflict), data=mydata[which(mydata$Format=='Qualified'),])
cohensD(confusion ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])    #d=0.76

#Bayesian t-tests for simple main effects
ttestBF(formula = confusion ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])

ttestBF(formula = confusion ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])




#advancement ANOVA
advancementlm <- lm(advancement ~ Conflict*Format, data = mydata)   #we first create a linear model 
Anova(advancementlm,type=3)    #we then use the car package calculates Anova (with type 3 SS) on the linear model we have just created  
cohens_f(advancementlm, partial = TRUE, ci = 0.95, squared = FALSE, model2 = NULL)

#Bayesian ANOVA 
advancement_bayes <- anovaBF(advancement ~ Conflict*Format, data = mydata)
advancement_bayes #lists the Bayes factor for each model against the null model

#whichModels = "all" computes Bayes factors for all models
advancement_bayes_all <- anovaBF(advancement ~ Conflict*Format, whichModels = "all", data = mydata)
advancement_bayes_all 


#Marginal means 
#means by Format

levels(mydata$Format)


#Compute summary statistics by groups


#contradiction
library(dplyr)
group_by(mydata, Format) %>%
  summarise(mean = mean(contradiction, na.rm = TRUE),
    sd = sd(contradiction, na.rm = TRUE))

#confusion
group_by(mydata, Format) %>%
  summarise(mean = mean(confusion, na.rm = TRUE),
    sd = sd(confusion, na.rm = TRUE))

#advancement
group_by(mydata, Format) %>%
  summarise(mean = mean(advancement, na.rm = TRUE),
    sd = sd(advancement, na.rm = TRUE))





#means by Conflict
levels(mydata$Conflict)


#Compute summary statistics by groups

#contradiction
library(dplyr)
group_by(mydata, Conflict) %>%
  summarise(mean = mean(contradiction, na.rm = TRUE),
            sd = sd(contradiction, na.rm = TRUE))

#confusion
group_by(mydata, Conflict) %>%
  summarise(mean = mean(confusion, na.rm = TRUE),
            sd = sd(confusion, na.rm = TRUE))

#advancement
group_by(mydata, Conflict) %>%
  summarise(mean = mean(advancement, na.rm = TRUE),
            sd = sd(advancement, na.rm = TRUE))



#Histograms
#for each of the four conditions showing how many people selected  -1 / 0 / +1

require(ggplot2)

#labelled each value
mydata$advancementvalue <- ordered(mydata$advancement,
                                   levels = c(-1, 0, 1),
                                   labels = c("Less", "Same", "More")) 

#option 1 which creates 4 graphs side by side 
ggplot(mydata, aes(x=advancement)) + # set x axis
  geom_histogram(color="black", fill="white", binwidth=1) + # set colours and binwidth
    facet_grid(~Conflict*Format) # set the four conditions



xAxisLabels <- c("Less","Same","More") #define discrete labels for x axis 

#option 2 puts all conditions in one graph

advancement_plot<-ggplot(mydata,aes(x=advancementvalue, group=Conflict:Format,fill=Conflict:Format)) + #set to group by 4 conditions
  geom_bar(position="dodge") +#put the conditions next to each other
   xlab("Advancement") + ylab("Number of Participants")+ #add titles
    scale_x_discrete(labels=xAxisLabels) #add discrete labels
    

advancement_plot<-advancement_plot+scale_fill_grey(name = "Condition", labels = c("Conflicting/Generic", "Conflicting/Qualified", "Non-conflicting/Generic", "Non-conflicting/Qualified")) #label legend


print(advancement_plot) #print with changed legend




#T-tests - Conduct one sample t-tests on the Advancement data  to compare the conflict and non-conflict means to a criterion of zero


conflictsubset<-subset(mydata, Conflict=="Conf.")

nonconflictsubset<-subset(mydata, Conflict=="Non-Conf.")


##run one sample t tests
#Generic group

#Conflict group
t.test(conflictsubset$advancement, mu = 0, alternative = "two.sided") #p-value = 0.00000961*

#Bayesian one-sample t-tests 
ttestBF(conflictsubset$advancement, mu = 0)

#non-conflict group
t.test(nonconflictsubset$advancement, mu = 0, alternative = "two.sided") #p-value = 0.9018

#Bayesian one-sample t-tests 
ttestBF(nonconflictsubset$advancement, mu = 0)


#Replicating  R1 effect size calculations 

#Effect size of Format (Generic vs Quantified) when the headlines were *Conflicting* 
#DV=Contradiction 
cohensD(contradiction ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Conf.'),])  #d=0.03
t.test(contradiction ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])   #p=0.84

#Bayesian t-tests contradiction 'conf'
ttestBF(formula = contradiction ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])


#DV=Advancement
cohensD(advancement ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Conf.'),])  #d=0.18
t.test(advancement ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])   #p=0.27

#Bayesian t-tests advancement 'conf'
ttestBF(formula = advancement ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])


#DV=Confusion  
cohensD(confusion ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Conf.'),])  #d=0.16
t.test(confusion ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])   #p=0.33

#Bayesian t-tests confusion 'conf'
ttestBF(formula = confusion ~ Format, data=mydata[which(mydata$Conflict=='Conf.'),])


#Effect size of Format (Generic vs Quantified) when the headlines were *Non-Conflicting* 
#DV=Contradiction 
cohensD(contradiction ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Non-Conf.'),])  #d=0.4
t.test(contradiction ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])   #p=0.018

#Bayesian t-tests contradiction 'non-conf'
ttestBF(formula = contradiction ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])


#DV=Advancement
cohensD(advancement ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Non-Conf.'),])  #d=0.27
t.test(advancement ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])   #p=0.11

#Bayesian t-tests advancement 'non-conf'
ttestBF(formula = advancement ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])


#DV=Confusion  
cohensD(confusion ~ as.factor(Format), data=mydata[which(mydata$Conflict=='Non-Conf.'),])  #d=0.26
t.test(confusion ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])   #p=0.12

#Bayesian t-tests confusion 'non-conf'
ttestBF(formula = confusion ~ Format, data=mydata[which(mydata$Conflict=='Non-Conf.'),])
#######





#Alternative method of calculating effect sizes  i.e., is the effect of Conflict smaller when headlines are quantified? (relative to when headlines are generic)


#DV=contradiction
#Effect size of Conflict (Conf vs Non-Conf) calculated seperately for Generic and Qualified headlines 
cohensD(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])  #Generic d=3.66
t.test(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])    #p<.001

#Bayesian t-tests contradiction 'generic'
ttestBF(formula = contradiction ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])


cohensD(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])  #Qualified d=2.74
t.test(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])    #p<.001

#Bayesian t-tests contradiction 'qualified'
ttestBF(formula = contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])



#add these effect sizes to relevant plots 
#DV=contradiction 
par(mfrow = c(2, 2))
pirateplot(formula = contradiction ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],inf.method = 'ci', yaxt = "n", ylim =c(0,30), theme=1,main = "Effect of Conflict on GENERIC headlines", ylab = "Percieved Contradiction",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 0, to = 30, by = 5)) 
text(x =c(1.5),y =c(20),labels =c("d=3.66"))

pirateplot(formula = contradiction ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],plot="FALSE")

pirateplot(formula = contradiction ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],inf.method = 'ci', yaxt = "n", ylim =c(0,30), theme=1,main = "Effect of Conflict on QUANT headlines", ylab = "Percieved Contradiction",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 0, to = 30, by = 5)) 
text(x =c(1.5),y =c(20),labels =c("d=2.74"))
par(mfrow = c(1, 1))

pirateplot(formula = contradiction ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],plot="FALSE")

#DV=advancement
#Effect size of Conflict (Conf vs Non-Conf) calculated seperately for Generic and Qualified headlines 
cohensD(advancement ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])  #Generic d= 0.41
t.test(advancement ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])    #p=0.014

#Bayesian t-tests advancement 'generic'
ttestBF(formula = advancement ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])


cohensD(advancement ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])  #Qualified d=0.34
t.test(advancement ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])    #p=0.034

#Bayesian t-tests advancement 'qualified'
ttestBF(formula = advancement ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])


#add these effect sizes to relevant plots 
par(mfrow = c(2, 2))
pirateplot(formula = advancement ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],inf.method = 'ci', yaxt = "n", ylim =c(-1,1), theme=1,main = "Effect of Conflict on GENERIC headlines", ylab = "Percieved Advancement",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = -1, to = 1, by = 1))
text(x =c(1.5),y =c(-0.5),labels =c("d=0.41"))


pirateplot(formula = advancement ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],plot="FALSE")

pirateplot(formula = advancement ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],inf.method = 'ci', yaxt = "n", ylim =c(-1,1), theme=1,main = "Effect of Conflict on QUANT headlines", ylab = "Percieved Advancement",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = -1, to = 1, by = 1))
text(x =c(1.5),y =c(-0.5),labels =c("d=0.34"))
par(mfrow = c(1, 1))

pirateplot(formula = advancement ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],plot="FALSE")




#DV=Confusion
#Effect size of Conflict (Conf vs Non-Conf) calculated seperately for Generic and Qualified headlines 
cohensD(confusion ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])  #Generic d=1.24
t.test(confusion ~ Conflict, data=mydata[which(mydata$Format=='Generic'),]) # p<.001

#Bayesian t-tests confusion 'generic'
ttestBF(formula = confusion ~ Conflict, data=mydata[which(mydata$Format=='Generic'),])


cohensD(confusion ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])  #Quantified d=0.76
t.test(contradiction ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),]) #p<.001

#Bayesian t-tests confusion 'qualified'
ttestBF(formula = confusion ~ Conflict, data=mydata[which(mydata$Format=='Qualified'),])


#add these effect sizes to relevant plots 
par(mfrow = c(2, 2))
pirateplot(formula = confusion ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],inf.method = 'ci', yaxt = "n", ylim =c(1,5), theme=1,main = "Effect of Conflict on GENERIC headlines", ylab = "Percieved Confusion",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 1, to = 5, by = 1)) 
text(x =c(1.4),y =c(3.75),labels =c("d=1.24"))

pirateplot(formula = confusion ~ Conflict*Format, data=mydata[which(mydata$Format=='Generic'),],plot="FALSE")

pirateplot(formula = confusion ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],inf.method = 'ci', yaxt = "n", ylim =c(1,5), theme=1,main = "Effect of Conflict on QUANT headlines", ylab = "Percieved Confusion",cex.names = 0.75, cex.lab = 0.9)
axis(2, at = seq(from = 1, to = 5, by = 1)) 
text(x =c(1.4),y =c(3.5),labels =c("d=0.76"))
par(mfrow = c(1, 1))


pirateplot(formula = confusion ~ Conflict*Format, data=mydata[which(mydata$Format=='Qualified'),],plot="FALSE")




####ORDINAL REGRESSION####
library("foreign") #load packages
library("ggplot2")
library("MASS")
library("Hmisc")
library("reshape2")

#assign a label to the value (this is already been done for advancement in the script above)
mydata$confusionvalue <- ordered(mydata$confusion,
    ##                               levels = c(1, 2, 3, 4, 5),
                                   labels = c("Strong disagree", "Somewhat disagree", "Neither agree nor disagree", "Somewhat agree", "Strongly agree")) 


#specify the reference category
mydata$Format <- relevel(mydata$Format, ref = "Generic") 
mydata$Conflict <- relevel(mydata$Conflict, ref = "Conf.") 


####CONFUSION ordinal regression
confusion_or <- polr(as.factor(confusionvalue) ~ Conflict * Format, data = mydata, Hess = TRUE)
confusion_or #print summary


#Compute confusion table and misclassification error
#The confusion matrix shows the performance of the ordinal logistic regression model
predict_confusion = predict(confusion_or, mydata)
table(mydata$confusionvalue, predict_confusion)
mean(as.character(mydata$confusionvalue) != as.character(predict_confusion))

#store table
(conf_table <- coef(summary(confusion_or)))

#calculate and store p values
conf_p <- pnorm(abs(conf_table[, "t value"]), lower.tail = FALSE) * 2

#combined table
(conf_table <- cbind(conf_table, "p value" = conf_p))

(conf_ci <- confint(confusion_or)) #default method gives profiled CIs
confint.default(confusion_or) #CIs assuming normality

#odds ratios
exp(coef(confusion_or))

#OR and CI
exp(cbind(OR = coef(confusion_or), conf_ci))




####ADVANCEMENT ordinal regression
advancement_or <- polr(as.factor(advancementvalue) ~ Conflict * Format, data = mydata, Hess = TRUE)
advancement_or #print summary


#Compute confusion table and misclassification error
#The confusion matrix shows the performance of the ordinal logistic regression model
predict_advancement = predict(advancement_or, mydata)
table(mydata$advancementvalue, predict_advancement)
mean(as.character(mydata$advancementvalue) != as.character(predict_advancement))

#store table
(adv_table <- coef(summary(advancement_or)))

#calculate and store p values
adv_p <- pnorm(abs(adv_table[, "t value"]), lower.tail = FALSE) * 2

#combined table
(adv_table <- cbind(adv_table, "p value" = adv_p))

(adv_ci <- confint(advancement_or)) # default method gives profiled CIs
confint.default(advancement_or) # CIs assuming normality

#odds ratios
exp(coef(advancement_or))

#OR and CI
exp(cbind(OR = coef(advancement_or), adv_ci))



